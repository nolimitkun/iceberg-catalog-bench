{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b04513-a43a-403d-a477-e5809bd979e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /usr/local/opt/python@3.10/bin/python3.10\n",
      "Version: 3.10.17 (main, Apr  8 2025, 12:10:59) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
      "Conda prefix: /opt/miniconda3/envs/iceberg-lab\n",
      "Platform: macOS-13.6.3-x86_64-i386-64bit\n",
      "Has pyspark? True\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pkgutil, platform\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Version:\", sys.version)\n",
    "print(\"Conda prefix:\", os.environ.get(\"CONDA_PREFIX\"))\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Has pyspark?\", bool(pkgutil.find_loader(\"pyspark\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9806ce9c-c9a5-4744-8fd2-9c5511395eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.1\n",
      "  Using cached pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.1)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488548 sha256=1045abdb62dcda73685f4474fbb653c557b11b9d0db427b0dbd38cc9121ffaaf\n",
      "  Stored in directory: /Users/yuan/Library/Caches/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing dependencies of pytorch-lightning: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.7.*)\n",
      "           ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyspark==3.5.1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886d5de-2c31-4cbf-bc41-6543a0dcb02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/yuan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/yuan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-azure-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0dcffd05-d567-4947-9ede-3d9c37bfad68;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-azure-bundle;1.4.1 in central\n",
      ":: resolution report :: resolve 140ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-azure-bundle;1.4.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0dcffd05-d567-4947-9ede-3d9c37bfad68\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n",
      "25/09/18 17:51:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#os.environ['SPARK_HOME'] = '/opt/miniconda3/envs/iceberg-lab/bin/pyspark'\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('iceberg_lab') \\\n",
    ".config(\"spark.executor.memory\", \"1g\") \\\n",
    ".config(\"spark.driver.memory\", \"8g\") \\\n",
    ".config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.1,org.apache.iceberg:iceberg-azure-bundle:1.4.1') \\\n",
    ".config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    ".config('spark.sql.defaultCatalog', 'opencatalog') \\\n",
    ".config('spark.sql.catalog.opencatalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.catalog.opencatalog.type', 'rest') \\\n",
    ".config('spark.sql.catalog.opencatalog.header.X-Iceberg-Access-Delegation','vended-credentials') \\\n",
    ".config('spark.sql.catalog.opencatalog.uri','https://pxlrpte-vs41448open.snowflakecomputing.com/polaris/api/catalog') \\\n",
    ".config('spark.sql.catalog.opencatalog.credential','4Qdi+HwwOAIBmt2zDe2s8LktPCc=:yoursecret') \\\n",
    ".config('spark.sql.catalog.opencatalog.warehouse','open_spark') \\\n",
    ".config('spark.sql.catalog.opencatalog.scope','PRINCIPAL_ROLE:spark') \\\n",
    ".getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ea317-a057-4575-ac50-13ddea7f34e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/yuan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/yuan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-azure-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3969d667-6d16-4869-909f-a49791fe1468;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-azure-bundle;1.4.1 in central\n",
      ":: resolution report :: resolve 150ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-azure-bundle;1.4.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3969d667-6d16-4869-909f-a49791fe1468\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n",
      "25/09/18 17:14:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('iceberg_lab') \\\n",
    ".config(\"spark.executor.memory\", \"1g\") \\\n",
    ".config(\"spark.driver.memory\", \"8g\") \\\n",
    ".config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.1,org.apache.iceberg:iceberg-azure-bundle:1.4.1') \\\n",
    ".config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    ".config('spark.sql.defaultCatalog', 'opencatalog') \\\n",
    ".config('spark.sql.catalog.opencatalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.catalog.opencatalog.type', 'rest') \\\n",
    ".config('spark.sql.catalog.opencatalog.header.X-Iceberg-Access-Delegation','vended-credentials') \\\n",
    ".config('spark.sql.catalog.opencatalog.uri','https://pxlrpte-vs41448open.snowflakecomputing.com/polaris/api/catalog') \\\n",
    ".config('spark.sql.catalog.opencatalog.credential','4Qdi+HwwOAIBmt2zDe2s8LktPCc=:yoursecret') \\\n",
    ".config('spark.sql.catalog.opencatalog.warehouse','open_spark_dfs') \\\n",
    ".config('spark.sql.catalog.opencatalog.scope','PRINCIPAL_ROLE:spark') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be6f7e-684e-4487-be75-b62892cc0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/yuan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/yuan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-azure-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d96501a0-166b-4247-bf52-04ff087456fe;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-azure-bundle;1.4.1 in central\n",
      ":: resolution report :: resolve 141ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-azure-bundle;1.4.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d96501a0-166b-4247-bf52-04ff087456fe\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n",
      "25/09/18 17:33:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('iceberg_lab') \\\n",
    ".config(\"spark.executor.memory\", \"1g\") \\\n",
    ".config(\"spark.driver.memory\", \"8g\") \\\n",
    ".config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.1,org.apache.iceberg:iceberg-azure-bundle:1.4.1') \\\n",
    ".config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    ".config('spark.sql.defaultCatalog', 'opencatalog') \\\n",
    ".config('spark.sql.catalog.opencatalog', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.catalog.opencatalog.type', 'rest') \\\n",
    ".config('spark.sql.catalog.opencatalog.header.X-Iceberg-Access-Delegation','vended-credentials') \\\n",
    ".config('spark.sql.catalog.opencatalog.uri','https://pxlrpte-vs41448open.snowflakecomputing.com/polaris/api/catalog') \\\n",
    ".config('spark.sql.catalog.opencatalog.credential','4Qdi+HwwOAIBmt2zDe2s8LktPCc=:yoursecret') \\\n",
    ".config('spark.sql.catalog.opencatalog.warehouse','open_spark_blob') \\\n",
    ".config('spark.sql.catalog.opencatalog.scope','PRINCIPAL_ROLE:spark') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df75254d-06f6-428e-a103-e1c706cd860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|open_spark|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show namespaces\n",
    "spark.sql(\"show namespaces\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95793c28-854c-4e64-abf1-667bb9f11bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create namespace\n",
    "spark.sql(\"create namespace open_spark_blob\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb1cfc95-339b-42f9-9150-14cf1956ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use namespace\n",
    "spark.sql(\"use namespace open_spark_blob\")\n",
    "\n",
    "#Show tables; this will show no tables since it is a new namespace\n",
    "spark.sql(\"show tables\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79b697b-2401-4f7f-b773-33a7a1cba3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a test table\n",
    "spark.sql(\"create table spark_table ( first_name STRING,last_name STRING,amount INT,create_date DATE) using iceberg partitioned by (first_name)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003f2404-8289-4adf-8c5f-97df88db30a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+-----------+\n",
      "|first_name|last_name|amount|create_date|\n",
      "+----------+---------+------+-----------+\n",
      "|       kun|      xue|   100| 2025-05-06|\n",
      "+----------+---------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#insert a record in the table\n",
    "spark.sql(\"insert into spark_table values ('kun', 'xue', 100, cast('2025-05-06'as date))\");\n",
    "\n",
    "#query the table\n",
    "spark.sql(\"select * from spark_table\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00155266-fd53-423a-b5d9-1cb4e1a2bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"phase\": \"write_append\",\n",
      "    \"target\": \"blob\",\n",
      "    \"seconds\": 556.3391714750032\n",
      "  },\n",
      "  {\n",
      "    \"phase\": \"read_pruned\",\n",
      "    \"target\": \"blob\",\n",
      "    \"seconds\": 15.744604509003693\n",
      "  },\n",
      "  {\n",
      "    \"phase\": \"read_agg\",\n",
      "    \"target\": \"blob\",\n",
      "    \"seconds\": 90.58831791300327\n",
      "  },\n",
      "  {\n",
      "    \"phase\": \"read_lookup\",\n",
      "    \"target\": \"blob\",\n",
      "    \"seconds\": 4.564584017993184\n",
      "  },\n",
      "  {\n",
      "    \"phase\": \"rewrite_data_files\",\n",
      "    \"target\": \"blob\",\n",
      "    \"seconds\": 0.9574697419884615\n",
      "  },\n",
      "  {\n",
      "    \"phase\": \"rewrite_manifests\",\n",
      "    \"target\": \"blob\",\n",
      "    \"seconds\": 0.0279179130011471\n",
      "  },\n",
      "  {\n",
      "    \"phase\": \"expire_snapshots\",\n",
      "    \"target\": \"blob\",\n",
      "    \"seconds\": 3.1421930130018154\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time, json, os, random\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "# ======= USER KNOBS =======\n",
    "TARGET = \"blob\"  # \"dfs\" or \"blob\"\n",
    "CATALOG = \"open_spark\"\n",
    "#TARGET = \"dfs\"\n",
    "#CATALOG = \"open_spark_dfs\"                      # matches spark.sql.catalog.<name>\n",
    "DB = \"bench\"\n",
    "TABLE = \"tx_events\"\n",
    "\n",
    "SCALE_ROWS = int(os.getenv(\"SCALE_ROWS\", 100_000_000))  # adjust for cluster; e.g., 100M ~ ~100GB depending on schema\n",
    "PARTITION_BY_DAYS = True\n",
    "BUCKETS = 16\n",
    "\n",
    "REPETITIONS = 3  # do each query 3x, use median\n",
    "\n",
    "# ==========================\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {CATALOG}.{DB}\")\n",
    "\n",
    "schema = T.StructType() \\\n",
    "    .add(\"user_id\", T.LongType()) \\\n",
    "    .add(\"ts\", T.TimestampType()) \\\n",
    "    .add(\"amount\", T.DoubleType()) \\\n",
    "    .add(\"city\", T.StringType()) \\\n",
    "    .add(\"category\", T.StringType())\n",
    "\n",
    "def synthesise(n):\n",
    "\n",
    "    # Deterministic synthetic generator\n",
    "    df = spark.range(n).withColumnRenamed(\"id\", \"user_id\")\n",
    "    \n",
    "    # 2025-01-01 00:00:00 UTC → epoch seconds\n",
    "    EPOCH_BASE = 1735689600\n",
    "    offset = (F.col(\"user_id\") % (60*60*24*30)).cast(\"long\")\n",
    "    df = df.withColumn(\"ts\",\n",
    "        F.to_timestamp(F.from_unixtime(F.lit(EPOCH_BASE) + offset))\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\"amount\", (F.rand(seed=42)*1000.0).cast(\"double\"))\n",
    "    cities = F.array([F.lit(c) for c in [\"Paris\",\"Seoul\",\"Tokyo\",\"Lyon\",\"Lille\",\"Marseille\",\"Nantes\",\"Bordeaux\"]])\n",
    "    cats = F.array([F.lit(c) for c in [\"A\",\"B\",\"C\",\"D\",\"E\"]])\n",
    "    df = df.withColumn(\"city\", cities[(F.col(\"user_id\") % F.size(cities)).cast(\"int\")]) \\\n",
    "           .withColumn(\"category\", cats[(F.col(\"user_id\") % F.size(cats)).cast(\"int\")])\n",
    "    return df.select(\"user_id\",\"ts\",\"amount\",\"city\",\"category\")\n",
    "\n",
    "table_ident = f\"{CATALOG}.{DB}.{TABLE}\"\n",
    "\n",
    "# Drop & (re)create table with same partition spec for both targets\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_ident}\")\n",
    "\n",
    "partition_spec = \"PARTITIONED BY (days(ts), bucket({b}, user_id))\".format(b=BUCKETS) if PARTITION_BY_DAYS else \"PARTITIONED BY (bucket({b}, user_id))\".format(b=BUCKETS)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE {table_ident} (\n",
    "    user_id BIGINT,\n",
    "    ts TIMESTAMP,\n",
    "    amount DOUBLE,\n",
    "    city STRING,\n",
    "    category STRING\n",
    "  )\n",
    "  USING iceberg\n",
    "  {partition_spec}\n",
    "  TBLPROPERTIES (\n",
    "    'write.target-file-size-bytes'='134217728',\n",
    "    'format-version'='2'\n",
    "  )\n",
    "\"\"\")\n",
    "\n",
    "def timer(fn):\n",
    "    t0 = time.perf_counter()\n",
    "    res = fn()\n",
    "    t1 = time.perf_counter()\n",
    "    return res, t1 - t0\n",
    "\n",
    "def median(xs):\n",
    "    s = sorted(xs)\n",
    "    n = len(s)\n",
    "    return 0 if n==0 else (s[n//2] if n%2==1 else 0.5*(s[n//2-1]+s[n//2]))\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1) WRITE (bulk append)\n",
    "df = synthesise(SCALE_ROWS).repartition(200)  # adjust to your cluster\n",
    "_, dur = timer(lambda: df.writeTo(table_ident).append())\n",
    "results.append({\"phase\":\"write_append\",\"target\":TARGET,\"seconds\":dur})\n",
    "\n",
    "# 2) READS (repeat 3x each, record median)\n",
    "def bench_sql(name, sql):\n",
    "    times = []\n",
    "    for _ in range(REPETITIONS):\n",
    "        _, d = timer(lambda: spark.sql(sql).agg(F.sum(\"cnt\")).collect() if \"cnt\" in sql else spark.sql(sql).collect())\n",
    "        times.append(d)\n",
    "    results.append({\"phase\":name,\"target\":TARGET,\"seconds\":median(times)})\n",
    "\n",
    "# Q1: Partition pruned (restrict to a couple of days)\n",
    "bench_sql(\"read_pruned\", f\"\"\"\n",
    "  SELECT city, count(*) as cnt\n",
    "  FROM {table_ident}\n",
    "  WHERE ts >= '2025-01-05' AND ts < '2025-01-07'\n",
    "  GROUP BY city\n",
    "\"\"\")\n",
    "\n",
    "# Q2: Wide aggregation\n",
    "bench_sql(\"read_agg\", f\"\"\"\n",
    "  SELECT category, approx_percentile(amount, 0.95) as p95, count(*) as cnt\n",
    "  FROM {table_ident}\n",
    "  GROUP BY category\n",
    "\"\"\")\n",
    "\n",
    "# Q3: High selectivity lookup\n",
    "bench_sql(\"read_lookup\", f\"\"\"\n",
    "  SELECT *\n",
    "  FROM {table_ident}\n",
    "  WHERE user_id IN (123, 456789, 987654321)\n",
    "\"\"\")\n",
    "\n",
    "# 3) MAINTENANCE\n",
    "# Compaction (data files)\n",
    "_, dur = timer(lambda: spark.sql(f\"\"\"\n",
    "  CALL opencatalog.system.rewrite_data_files(table => '{table_ident}', options => map('min-input-files','50','max-file-size-bytes','536870912'))\n",
    "\"\"\").collect())\n",
    "results.append({\"phase\":\"rewrite_data_files\",\"target\":TARGET,\"seconds\":dur})\n",
    "\n",
    "# Rewrite manifests\n",
    "_, dur = timer(lambda: spark.sql(f\"\"\"\n",
    "  CALL opencatalog.system.rewrite_manifests('{table_ident}')\n",
    "\"\"\").collect())\n",
    "results.append({\"phase\":\"rewrite_manifests\",\"target\":TARGET,\"seconds\":dur})\n",
    "\n",
    "# Expire snapshots (keep last 2)\n",
    "_, dur = timer(lambda: spark.sql(f\"\"\"\n",
    "  CALL opencatalog.system.expire_snapshots(table => '{table_ident}', retain_last => 2)\n",
    "\"\"\").collect())\n",
    "results.append({\"phase\":\"expire_snapshots\",\"target\":TARGET,\"seconds\":dur})\n",
    "\n",
    "# Save results (CSV)\n",
    "out_path = f\"/tmp/iceberg_ab_{TARGET}_results.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9871602-69fa-4241-ac05-2f8e19fe58db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
